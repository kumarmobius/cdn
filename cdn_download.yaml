name: CDN Download
description: |
  Downloads trained_model.pt, config.properties, and tokenizer.json files from given CDN URLs
  and saves them locally with verified directory structure and file integrity checks.

inputs:
  - {name: pt_url, type: String, description: "URL to fetch the trained_model.pt model file from"}
  - {name: config_url, type: String, description: "URL to fetch the config.properties file from"}
  - {name: tokenizer_url, type: String, description: "URL to fetch the tokenizer.json file from"}

outputs:
  - {name: pt_file, type: Model, description: "Directory artifact containing trained_model.pt"}
  - {name: config_file, type: Data, description: "Directory artifact containing config.properties"}
  - {name: tokenizer_file, type: Model, description: "Directory artifact containing tokenizer.json"}

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        set -e
        echo "Installing dependencies..."
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet requests || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet requests --user
        exec python3 -u - "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import requests
        import sys

        CHUNK_SIZE = 16 * 1024

        parser = argparse.ArgumentParser()
        parser.add_argument('--pt_url', type=str, required=True)
        parser.add_argument('--pt_file', type=str, required=True)       # treated as directory
        parser.add_argument('--config_url', type=str, required=True)
        parser.add_argument('--config_file', type=str, required=True)   # treated as directory
        parser.add_argument('--tokenizer_url', type=str, required=True)
        parser.add_argument('--tokenizer_file', type=str, required=True) # treated as directory
        args = parser.parse_args()

        def makedirs(p):
            os.makedirs(p, exist_ok=True)

        def download_stream_to_file(url, out_file, timeout=300):
            print(f"Downloading {url} -> {out_file}")
            # ensure parent exists
            makedirs(os.path.dirname(out_file) or ".")
            try:
                with requests.get(url, stream=True, timeout=timeout) as r:
                    r.raise_for_status()
                    total = 0
                    with open(out_file, "wb") as fh:
                        for chunk in r.iter_content(chunk_size=CHUNK_SIZE):
                            if chunk:
                                fh.write(chunk)
                                total += len(chunk)
                print(f"Saved {out_file} ({total} bytes)")
                return total
            except requests.exceptions.RequestException as e:
                print(f"Network error while downloading {url}: {e}")
                raise

        try:
            print("pt_url:", args.pt_url)
            print("config_url:", args.config_url)
            print("tokenizer_url:", args.tokenizer_url)

            # Decide actual file paths inside the provided output directories.
            # The pipeline runtime typically gives a directory path for output artifacts;
            # we will write known filenames inside those directories so the runtime can archive them.
            pt_dir = args.pt_file
            cfg_dir = args.config_file
            tok_dir = args.tokenizer_file

            # Ensure they are directories (create them)
            makedirs(pt_dir)
            makedirs(cfg_dir)
            makedirs(tok_dir)

            pt_out = os.path.join(pt_dir, "trained_model.pt")
            cfg_out = os.path.join(cfg_dir, "config.properties")
            tok_out = os.path.join(tok_dir, "tokenizer.json")

            # Download files into those paths
            download_stream_to_file(args.pt_url, pt_out, timeout=600)
            download_stream_to_file(args.config_url, cfg_out, timeout=120)
            download_stream_to_file(args.tokenizer_url, tok_out, timeout=120)

            # Basic verification
            for fpath, label in [
                (pt_out, "PT model"),
                (cfg_out, "Config file"),
                (tok_out, "Tokenizer file"),
            ]:
                if not (os.path.exists(fpath) and os.path.getsize(fpath) > 0):
                    print(f"Error: {label} missing or empty: {fpath}")
                    sys.exit(1)
                else:
                    print(f"Verified {label}: {fpath} ({os.path.getsize(fpath)} bytes)")

            print(" All files downloaded and verified successfully.")
            print("Artifacts produced:")
            print(" - PT dir:", pt_dir)
            print(" - Config dir:", cfg_dir)
            print(" - Tokenizer dir:", tok_dir)

        except Exception as e:
            print(" Error during download:", str(e))
            import traceback
            traceback.print_exc()
            sys.exit(1)
    args:
      - --pt_url
      - {inputValue: pt_url}
      - --pt_file
      - {outputPath: pt_file}
      - --config_url
      - {inputValue: config_url}
      - --config_file
      - {outputPath: config_file}
      - --tokenizer_url
      - {inputValue: tokenizer_url}
      - --tokenizer_file
      - {outputPath: tokenizer_file}
