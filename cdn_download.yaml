name: CDN Download Files
description: Downloads trained_model.pt, config.json, and tokenizer.json files from CDN URLs and saves them locally with proper directory structure.
inputs:
  - {name: pt_url, type: String, description: "URL to fetch the trained_model.pt model file from"}
  - {name: config_url, type: String, description: "URL to fetch the config.json file from"}
  - {name: tokenizer_url, type: String, description: "URL to fetch the tokenizer.json file from"}
outputs:
  - {name: pt_file, type: Model, description: "Downloaded trained_model.pt model file"}
  - {name: config_file, type: Data, description: "Downloaded config.json file"}
  - {name: tokenizer_file, type: Model, description: "Downloaded tokenizer.json file"}
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet requests || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet requests --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import requests
        import sys

        CHUNK_SIZE = 16 * 1024

        parser = argparse.ArgumentParser()
        parser.add_argument('--pt_url', type=str, required=True)
        parser.add_argument('--pt_file', type=str, required=True)
        parser.add_argument('--config_url', type=str, required=True)
        parser.add_argument('--config_file', type=str, required=True)
        parser.add_argument('--tokenizer_url', type=str, required=True)
        parser.add_argument('--tokenizer_file', type=str, required=True)

        args = parser.parse_args()

        def makedirs(p):
            if p and not os.path.exists(p):
                os.makedirs(p, exist_ok=True)

        def resolve_output_path(output_arg, default_filename):
            """
            If the runtime passed a path whose basename is 'data' (common in KFP/Argo),
            treat that as the final file path and return it.
            Otherwise treat output_arg as a directory and return dir/default_filename.
            """
            base = os.path.basename(output_arg)
            if base == "data":
                makedirs(os.path.dirname(output_arg) or ".")
                return output_arg
            else:
                makedirs(output_arg)
                return os.path.join(output_arg, default_filename)

        def download_stream_to_file(url, out_file, timeout=300):
            print(f"Downloading {url} -> {out_file}")
            makedirs(os.path.dirname(out_file) or ".")
            try:
                with requests.get(url, stream=True, timeout=timeout) as r:
                    r.raise_for_status()
                    total = 0
                    with open(out_file, "wb") as fh:
                        for chunk in r.iter_content(chunk_size=CHUNK_SIZE):
                            if chunk:
                                fh.write(chunk)
                                total += len(chunk)
                print(f"Saved {out_file} ({total} bytes)")
                return total
            except requests.exceptions.RequestException as e:
                print(f"Network error while downloading {url}: {e}")
                raise

        try:
            print("pt_url:", args.pt_url)
            print("config_url:", args.config_url)
            print("tokenizer_url:", args.tokenizer_url)

            # Resolve final output paths (support both directory-style and runtime 'data' file)
            pt_out = resolve_output_path(args.pt_file, "trained_model.pt")
            cfg_out = resolve_output_path(args.config_file, "config.json")
            tok_out = resolve_output_path(args.tokenizer_file, "tokenizer.json")

            # Download files into those paths
            download_stream_to_file(args.pt_url, pt_out, timeout=600)
            download_stream_to_file(args.config_url, cfg_out, timeout=120)
            download_stream_to_file(args.tokenizer_url, tok_out, timeout=120)

            # Basic verification
            for fpath, label in [
                (pt_out, "PT model"),
                (cfg_out, "Config file"),
                (tok_out, "Tokenizer file"),
            ]:
                if not (os.path.exists(fpath) and os.path.getsize(fpath) > 0):
                    print(f"Error: {label} missing or empty: {fpath}")
                    sys.exit(1)
                else:
                    print(f"Verified {label}: {fpath} ({os.path.getsize(fpath)} bytes)")

            print("All files downloaded and verified successfully.")
            print("Artifacts produced:")
            print(" - PT path:", pt_out)
            print(" - Config path:", cfg_out)
            print(" - Tokenizer path:", tok_out)

        except Exception as e:
            print("Error during download:", str(e))
            import traceback
            traceback.print_exc()
            sys.exit(1)
    args:
      - --pt_url
      - {inputValue: pt_url}
      - --pt_file
      - {outputPath: pt_file}
      - --config_url
      - {inputValue: config_url}
      - --config_file
      - {outputPath: config_file}
      - --tokenizer_url
      - {inputValue: tokenizer_url}
      - --tokenizer_file
      - {outputPath: tokenizer_file}
